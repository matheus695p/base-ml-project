{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext jupyter_black\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic use case\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and model imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.packages.preprocessing.transformers.raw import RawDataProcessor\n",
    "from project.packages.preprocessing.transformers.intermediate import (\n",
    "    IntermediateDataProcessor,\n",
    ")\n",
    "from project.packages.preprocessing.transformers.primary import PrimaryDataProcessor\n",
    "from project.packages.preprocessing.transformers.feature import FeatureDataProcessor\n",
    "from project.packages.modelling.models.unsupervised.clustering_features import (\n",
    "    KMeansClusteringFeatures,\n",
    ")\n",
    "from project.packages.modelling.models.supervised.sklearn import (\n",
    "    BinaryClassifierSklearnPipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Titanic dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/01_raw/titanic_train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "### Raw data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_params = {\n",
    "    \"target\": \"Survived\",\n",
    "    \"index\": \"passenger_id\",\n",
    "    \"schemas\": {\n",
    "        \"PassengerId\": {\"dtype\": \"int64\", \"name\": \"passenger_id\"},\n",
    "        \"Survived\": {\"dtype\": \"int64\", \"name\": \"survived\"},\n",
    "        \"Pclass\": {\"dtype\": \"int64\", \"name\": \"passenger_class\"},\n",
    "        \"Name\": {\"dtype\": \"object\", \"name\": \"name\"},\n",
    "        \"Sex\": {\"dtype\": \"object\", \"name\": \"passenger_sex\"},\n",
    "        \"Age\": {\"dtype\": \"float64\", \"name\": \"passenger_age\"},\n",
    "        \"Parch\": {\"dtype\": \"int64\", \"name\": \"passenger_parch\"},\n",
    "        \"Ticket\": {\"dtype\": \"object\", \"name\": \"passenger_ticket\"},\n",
    "        \"Fare\": {\"dtype\": \"float64\", \"name\": \"passenger_fare\"},\n",
    "        \"Cabin\": {\"dtype\": \"object\", \"name\": \"passenger_cabin\"},\n",
    "        \"Embarked\": {\"dtype\": \"object\", \"name\": \"passenger_embarked_port\"},\n",
    "        \"SibSp\": {\"dtype\": \"int64\", \"name\": \"passenger_siblings\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "raw_transformer = RawDataProcessor(raw_params)\n",
    "df_raw = raw_transformer.fit_transform(df)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate data preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_params = {\n",
    "    \"target\": \"survived\",\n",
    "    \"outlier_params\": {\"iqr_alpha\": 2.5, \"q1_quantile\": 0.25, \"q3_quantile\": 0.75},\n",
    "    \"drop_columns\": [\"name\"],\n",
    "    \"categorical_features\": [\n",
    "        \"passenger_sex\",\n",
    "        \"passenger_ticket\",\n",
    "        \"passenger_cabin\",\n",
    "        \"passenger_embarked_port\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "int_transformer = IntermediateDataProcessor(intermediate_params)\n",
    "df_int = int_transformer.fit_transform(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_params = {\n",
    "    \"target\": \"supervised\",\n",
    "    \"categorical_columns_fillna\": {\n",
    "        \"passenger_cabin\": \"unknown\",\n",
    "        \"passenger_embarked_port\": \"unknown\",\n",
    "    },\n",
    "}\n",
    "prm_transformer = PrimaryDataProcessor(primary_params)\n",
    "df_prm = prm_transformer.fit_transform(df_int)\n",
    "df_prm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering \n",
    "\n",
    "#### 1. Encoding and specific features data creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_params = {\n",
    "    \"target\": \"survived\",\n",
    "    \"encoding_transform\": {\n",
    "        \"one_hot_encoder\": [\n",
    "            \"passenger_cabin_level\",\n",
    "            \"passenger_embarked_port\",\n",
    "            \"passenger_sex\",\n",
    "        ],\n",
    "        \"similarity_based_encoder\": None,\n",
    "    },\n",
    "}\n",
    "feat_transformer = FeatureDataProcessor(feature_params)\n",
    "df_feat = feat_transformer.fit_transform(df_prm)\n",
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model_params = {\n",
    "    \"class\": \"project.packages.modelling.models.unsupervised.segmentation.KMeansElbowSelector\",\n",
    "    \"kwargs\": {\"min_clusters\": 1, \"max_clusters\": 15},\n",
    "}\n",
    "cluster_scaler_params = {\n",
    "    \"class\": \"project.packages.modelling.transformers.scaler.ColumnsPreserverScaler\",\n",
    "    \"kwargs\": {\n",
    "        \"scaler_params\": {\"class\": \"sklearn.preprocessing.MinMaxScaler\", \"kwargs\": {}}\n",
    "    },\n",
    "}\n",
    "cluster_imputer_params = {\n",
    "    \"class\": \"project.packages.modelling.models.unsupervised.imputer.ColumnsPreserverImputer\",\n",
    "    \"kwargs\": {\n",
    "        \"imputer_params\": {\n",
    "            \"class\": \"sklearn.impute.KNNImputer\",\n",
    "            \"kwargs\": {\"n_neighbors\": 10, \"weights\": \"distance\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# cluster feature name and features used to create the cluster feature\n",
    "cluster_feature_params = {\n",
    "    \"passenger_cabin_cluster_feature\": [\n",
    "        \"passenger_cabin_level_a\",\n",
    "        \"passenger_cabin_level_b\",\n",
    "        \"passenger_cabin_level_c\",\n",
    "        \"passenger_cabin_level_d\",\n",
    "        \"passenger_cabin_level_e\",\n",
    "        \"passenger_cabin_level_f\",\n",
    "        \"passenger_cabin_level_g\",\n",
    "        \"passenger_cabin_level_t\",\n",
    "        \"passenger_cabin_level_unknown\",\n",
    "    ],\n",
    "    \"passenger_embarked_port_cluster_feature\": [\n",
    "        \"passenger_embarked_port_c\",\n",
    "        \"passenger_embarked_port_q\",\n",
    "        \"passenger_embarked_port_s\",\n",
    "        \"passenger_embarked_port_unknown\",\n",
    "    ],\n",
    "    \"passenger_ticket_number_cluster_feature\": [\n",
    "        \"passenger_ticket_number\",\n",
    "        \"passenger_ticket_unknown_base\",\n",
    "    ],\n",
    "    \"passenger_family_cluster_feature\": [\n",
    "        \"passenger_siblings\",\n",
    "        \"passenger_parch\",\n",
    "        \"passenger_cabin_number\",\n",
    "        \"passenger_number_of_family_onboard\",\n",
    "    ],\n",
    "    \"passenger_social_status_cluster_feature\": [\n",
    "        \"passenger_class\",\n",
    "        \"passenger_age\",\n",
    "        \"passenger_sex_female\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "cluster_transformer = KMeansClusteringFeatures(\n",
    "    model_params=cluster_model_params,\n",
    "    scaler_params=cluster_scaler_params,\n",
    "    feature_params=cluster_feature_params,\n",
    "    imputer_params=cluster_imputer_params,\n",
    ")\n",
    "data = cluster_transformer.fit_transform(df_feat)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data engineering in a single sklearn pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"raw_transformations\", RawDataProcessor(raw_params)),\n",
    "        (\n",
    "            \"intermediate_transformations\",\n",
    "            IntermediateDataProcessor(intermediate_params),\n",
    "        ),\n",
    "        (\"primary_transformations\", PrimaryDataProcessor(primary_params)),\n",
    "        (\"feature_transformations\", FeatureDataProcessor(feature_params)),\n",
    "        (\n",
    "            \"cluster_feature_transformations\",\n",
    "            KMeansClusteringFeatures(\n",
    "                model_params=cluster_model_params,\n",
    "                scaler_params=cluster_scaler_params,\n",
    "                feature_params=cluster_feature_params,\n",
    "                imputer_params=cluster_imputer_params,\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "data = pipeline.fit_transform(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hypertune and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"scoring_metrics\": [\n",
    "        \"accuracy\",\n",
    "        \"balanced_accuracy\",\n",
    "        \"f1\",\n",
    "        \"f1_micro\",\n",
    "        \"f1_macro\",\n",
    "        \"f1_weighted\",\n",
    "        \"precision\",\n",
    "        \"precision_micro\",\n",
    "        \"precision_macro\",\n",
    "        \"precision_weighted\",\n",
    "        \"recall\",\n",
    "        \"recall_micro\",\n",
    "        \"recall_macro\",\n",
    "        \"recall_weighted\",\n",
    "        \"roc_auc\",\n",
    "        \"roc_auc_ovr\",\n",
    "        \"roc_auc_ovo\",\n",
    "        \"roc_auc_ovr_weighted\",\n",
    "        \"roc_auc_ovo_weighted\",\n",
    "    ],\n",
    "    \"optuna\": {\n",
    "        \"kwargs_study\": {\n",
    "            \"direction\": \"maximize\",\n",
    "            \"study_name\": \"xgboost\",\n",
    "            \"load_if_exists\": False,\n",
    "        },\n",
    "        \"kwargs_optimize\": {\"n_trials\": 500},\n",
    "        \"sampler\": {\n",
    "            \"class\": \"optuna.samplers.TPESampler\",\n",
    "            \"kwargs\": {\"n_startup_trials\": 0, \"constant_liar\": True, \"seed\": 42},\n",
    "        },\n",
    "        \"pruner\": {\"class\": \"optuna.pruners.SuccessiveHalvingPruner\", \"kwargs\": {}},\n",
    "    },\n",
    "    \"cv_strategy\": {\n",
    "        \"class\": \"sklearn.model_selection.StratifiedKFold\",\n",
    "        \"kwargs\": {\"n_splits\": 5, \"random_state\": 42, \"shuffle\": True},\n",
    "    },\n",
    "    \"cv_score\": {\n",
    "        \"scoring\": \"f1_weighted\",\n",
    "        \"class\": \"sklearn.model_selection.cross_val_predict\",\n",
    "        \"kwargs\": {\n",
    "            \"estimator\": None,\n",
    "            \"X\": None,\n",
    "            \"y\": None,\n",
    "            \"cv\": None,\n",
    "            \"n_jobs\": -1,\n",
    "            \"method\": \"predict\",\n",
    "        },\n",
    "    },\n",
    "    \"target\": \"survived\",\n",
    "    \"features\": [\n",
    "        \"passenger_class\",\n",
    "        \"passenger_age\",\n",
    "        \"passenger_siblings\",\n",
    "        \"passenger_parch\",\n",
    "        \"passenger_fare\",\n",
    "        \"passenger_ticket_number\",\n",
    "        \"passenger_ticket_unknown_base\",\n",
    "        \"passenger_cabin_number\",\n",
    "        \"passenger_number_of_family_onboard\",\n",
    "        \"passenger_is_single\",\n",
    "        \"passenger_has_childs\",\n",
    "        \"passenger_cabin_level_a\",\n",
    "        \"passenger_cabin_level_b\",\n",
    "        \"passenger_cabin_level_c\",\n",
    "        \"passenger_cabin_level_d\",\n",
    "        \"passenger_cabin_level_e\",\n",
    "        \"passenger_cabin_level_unknown\",\n",
    "        \"passenger_embarked_port_c\",\n",
    "        \"passenger_embarked_port_q\",\n",
    "        \"passenger_embarked_port_s\",\n",
    "        \"passenger_sex_female\",\n",
    "        \"passenger_cabin_cluster_feature\",\n",
    "        \"passenger_embarked_port_cluster_feature\",\n",
    "        \"passenger_ticket_number_cluster_feature\",\n",
    "        \"passenger_family_cluster_feature\",\n",
    "        \"passenger_social_status_cluster_feature\",\n",
    "    ],\n",
    "    \"pipeline\": {\n",
    "        \"imputer\": {\n",
    "            \"class\": \"project.packages.modelling.models.unsupervised.imputer.ColumnsPreserverImputer\",\n",
    "            \"kwargs\": {\n",
    "                \"imputer_params\": {\n",
    "                    \"class\": \"sklearn.impute.KNNImputer\",\n",
    "                    \"kwargs\": {\n",
    "                        \"n_neighbors\": 'trial.suggest_int(\"knn_imputer__n_neighbors\", 2, 20, step=1)',\n",
    "                        \"weights\": 'trial.suggest_categorical(\"knn_imputer__weights\", [\"distance\", \"uniform\"])',\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"scaler\": {\n",
    "            \"class\": \"project.packages.modelling.transformers.scaler.ColumnsPreserverScaler\",\n",
    "            \"kwargs\": {\n",
    "                \"scaler_params\": {\n",
    "                    \"class\": 'trial.suggest_categorical(\"scaler__transformer\", [\"project.packages.modelling.transformers.scaler.NotScalerTransformer\", \"sklearn.preprocessing.PowerTransformer\", \"sklearn.preprocessing.QuantileTransformer\"])',\n",
    "                    \"kwargs\": {},\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"class\": \"xgboost.XGBClassifier\",\n",
    "            \"kwargs\": {\n",
    "                \"n_estimators\": 'trial.suggest_int(\"xgboost__n_estimators\", 10, 500, step=5)',\n",
    "                \"learning_rate\": 'trial.suggest_float(\"xgboost__learning_rate\", 0.0001, 1)',\n",
    "                \"min_child_weight\": 'trial.suggest_int(\"xgboost__min_child_weight\", 0, 500, step=1)',\n",
    "                \"max_depth\": 'trial.suggest_int(\"xgboost__max_depth\", 1, 8)',\n",
    "                \"subsample\": 'trial.suggest_float(\"xgboost__subsample\", 0.5, 1)',\n",
    "                \"reg_lambda\": 'trial.suggest_float(\"xgboost__reg_lambda\", 0, 5)',\n",
    "                \"reg_alpha\": 'trial.suggest_float(\"xgboost__reg_alpha\", 0, 1)',\n",
    "                \"random_state\": 42,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "target = \"survived\"\n",
    "model = BinaryClassifierSklearnPipeline(model_params)\n",
    "\n",
    "y_train = data[[target]]\n",
    "X_train = data[[col for col in data.columns if col != target]]\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(data)\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al process in a single sklearn Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"raw_transformations\", RawDataProcessor(raw_params)),\n",
    "        (\n",
    "            \"intermediate_transformations\",\n",
    "            IntermediateDataProcessor(intermediate_params),\n",
    "        ),\n",
    "        (\"primary_transformations\", PrimaryDataProcessor(primary_params)),\n",
    "        (\"feature_transformations\", FeatureDataProcessor(feature_params)),\n",
    "        (\n",
    "            \"cluster_feature_transformations\",\n",
    "            KMeansClusteringFeatures(\n",
    "                model_params=cluster_model_params,\n",
    "                scaler_params=cluster_scaler_params,\n",
    "                feature_params=cluster_feature_params,\n",
    "                imputer_params=cluster_imputer_params,\n",
    "            ),\n",
    "        ),\n",
    "        (\"model\", BinaryClassifierSklearnPipeline(model_params)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package CLI\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to the Package CLI readme. This comprehensive guide provides an in-depth understanding of the Package CLI, its structure, and its capabilities. The Package CLI is a powerful tool designed to encapsulate Kedro pipelines for a wide range of machine learning tasks, enabling streamlined data processing, model training, evaluation, and deployment.\n",
    "\n",
    "## 1. Introduction<a name=\"introduction\"></a>\n",
    "\n",
    "The Package CLI simplifies complex machine learning workflows by breaking them down into modular pipelines. It seamlessly integrates with Kedro, a data engineering framework, to provide a structured approach to ML project development.\n",
    "\n",
    "## 2. Package Structure<a name=\"package-structure\"></a>\n",
    "\n",
    "The package is organized as follows:\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "- Pipelines are the backbone of the Package CLI. Each step in the ML workflow is broken down into separate pipelines. This modular approach improves code readability and maintainability.\n",
    "\n",
    "### Logging and Tracking\n",
    "\n",
    "- The Package CLI utilizes MLflow for logging and tracking experiments. This integration enables comprehensive monitoring of your ML projects, including model performance, data lineage, and hyperparameter optimization.\n",
    "\n",
    "## 3. Logging and Tracking<a name=\"logging-and-tracking\"></a>\n",
    "\n",
    "MLflow is at the core of our logging and tracking system:\n",
    "\n",
    "- **Data**: Input data, transformed data, and data splits are logged to ensure complete traceability.\n",
    "- **Model Artifacts**: Serialized model files are logged, simplifying model replication and deployment.\n",
    "- **Metrics**: Key performance metrics, such as accuracy, precision, recall, and F1-score, are tracked.\n",
    "- **Hyperparameters**: Detailed information about the hyperparameters used during training is recorded.\n",
    "- **Experiment Parameters**: Parameters set for each experiment run are logged for easy reproducibility.\n",
    "- **Models reprotign**: All html files that reports, performance reports, hyperparameters study, model predictive control exploration and global model optimization reports are logged as artifacts.\n",
    "\n",
    "## 4. Custom Pipelines<a name=\"custom-pipelines\"></a>\n",
    "\n",
    "The Package CLI provides a collection of custom Kedro pipelines, making it easy to integrate into your ML projects. These pipelines cover fundamental steps in the ML workflow, including:\n",
    "\n",
    "- Data preprocessing and feature engineering.\n",
    "- Model training and evaluation.\n",
    "- Model deployment and serving.\n",
    "\n",
    "These pipelines are designed to be highly modular, allowing you to extend or customize them to meet your specific project requirements.\n",
    "\n",
    "## 5. Model Compatibility<a name=\"model-compatibility\"></a>\n",
    "\n",
    "The Package CLI includes the `BinaryClassifierSklearnPipeline` class, which is compatible with any machine learning model adhering to the scikit-learn API. This flexibility empowers you to experiment with a wide variety of models, including:\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Support Vector Machines\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "- Xgboost\n",
    "- SVM\n",
    "- k-NN\n",
    "\n",
    "And all compatible models\n",
    "\n",
    "## 6. Hyperparameter Tuning<a name=\"hyperparameter-tuning\"></a>\n",
    "\n",
    "Using the kedro cli. You can explore different hyperparameter settings for different models to optimize its performance, using **StratifiedKFold** cross validation strategy. Hyperparameter tuning is seamlessly integrated into the pipeline, simplifying the search for the best model configurations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engineering CLI<a name=\"data-engineering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kedro run --pipeline data_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data engineering pipelines ends running from raw to cluster transformers and save train and test datasets ready to be used by models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Data Science Pipelines project comprises 9 modular pipelines, each uniquely designed to optimize different machine learning models. These pipelines are thoughtfully constructed to leverage the full potential of your data and to achieve the highest possible model performance.\n",
    "\n",
    "## Model Optimization\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Our pipelines explore a diverse range of models, including:\n",
    "\n",
    "- Bagging Models\n",
    "- Boosting Models\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Neural Network Models\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "To ensure that these models perform at their best, we employ hyperparameter tuning using cross-validation strategies. This rigorous process fine-tunes the model parameters to maximize predictive accuracy and generalization.\n",
    "\n",
    "### Saving Trained Models\n",
    "\n",
    "Once the models are optimized, they are trained on the entire dataset. These trained models are diligently saved for future use, facilitating easy inference on new data and seamless deployment in production environments.\n",
    "\n",
    "## Out-of-Sample Predictions\n",
    "\n",
    "To evaluate model performance, we generate out-of-sample predictions using the `cross_val_predict` class from scikit-learn. These predictions provide invaluable insights into how well the models generalize to unseen data.\n",
    "\n",
    "## Metrics Reporting\n",
    "\n",
    "The final segment of the pipelines involves the creation of a comprehensive metrics report. This report encompasses key performance metrics, enabling you to thoroughly assess the models' effectiveness. Commonly included metrics are accuracy, precision, recall, F1-score, and more.\n",
    "\n",
    "## Production Deployment with MLflow\n",
    "\n",
    "One of the standout features of these pipelines is their seamless integration with MLflow. The best-performing model is automatically registered in a production environment within MLflow. This production-ready model is primed for deployment and can be employed to perform inference on a test dataset. It plays a pivotal role in ranking models based on their performance, ensuring that only the top-performing models are deployed in production.\n",
    "\n",
    "In conclusion, the Data Science Pipelines provided in this project offer a comprehensive and efficient means to optimize machine learning models and rigorously assess their performance. With modular pipelines, hyperparameter tuning, out-of-sample predictions, metrics reporting, and seamless integration with MLflow for production deployment, these pipelines are an invaluable asset for data scientists and machine learning practitioners. Utilize them to supercharge your data science projects and achieve exceptional model results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kedro run --pipeline data_science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow exploration\n",
    "\n",
    "\n",
    "After running data engineering and data science pipelines you can access to the mlflow UI using the following command you should be able to see this url:\n",
    "\n",
    "- **http://127.0.0.1:3001** \n",
    "\n",
    "Open it in a browser and navigate to the artifacts, metrics, models and registered models folder.\n",
    "\n",
    "**Artifacts**: This section houses a variety of assets, including input data, transformed data, model files, and other relevant files. It provides a comprehensive view of the artifacts generated during your experiments.\n",
    "\n",
    "**Metrics**: In the \"Metrics\" section, you can review key performance metrics recorded during your experiments. These metrics are crucial for assessing the effectiveness of your models and pipelines. Commonly logged metrics include accuracy, precision, recall, F1-score, and more.\n",
    "\n",
    "**Models**: Explore the models that have been trained and logged during your experiments in the \"Models\" section. You can access detailed information about each model, including its hyperparameters, performance metrics, and any associated artifacts.\n",
    "\n",
    "**Registered** Models: The \"Registered Models\" folder contains the best-performing models that have been specifically registered for production deployment. These models have undergone rigorous evaluation and are deemed ready for use in real-world applications. This section provides a streamlined view of models that meet the highest quality standards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kedro mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model productionalization\n",
    "\n",
    "After reviweing MLflow models, next step is put these model on production through an API, so package also contains API version to the production model and transformers used to create these model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the API on production\n",
    "\n",
    "Excecute the following command to have a POST endpoint with the production model.\n",
    "\n",
    "!python src/project/api/model_serving/app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/project/api/model_serving/app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model API\n",
    "\n",
    "!python src/project/api/model_serving/ping_api.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/project/api/model_serving/ping_api.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing the API you should recieve this text on the terminal\n",
    "\n",
    "\n",
    "**Model response: Matheus Pinto Arratia has a survival probability of 30.87 [%]**\n",
    "\n",
    "So, I would probably die ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package testing \n",
    "\n",
    "You can test the whole package using the following command\n",
    "\n",
    "!kedro test src/project/packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the hole package testing and modules that are missing to be tested\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About CI/CD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Deployment Architectures\n",
    "\n",
    "When considering deploying these applications into production, there are several architectural options to choose from, each with its own set of advantages and disadvantages.\n",
    "\n",
    "## Gitflow\n",
    "\n",
    "**Pros:**\n",
    "- Ensures a well-structured development process with clear branching strategies.\n",
    "- Continuous Integration (CI) pipeline ensures code quality and functionality.\n",
    "- Integration tests and unit tests provide end-to-end testing coverage.\n",
    "- Code versioning and history tracking.\n",
    "\n",
    "## Docker\n",
    "\n",
    "API, pipelines and packages should be containarized using docker, in order to maintain reproducibility\n",
    "\n",
    "**Pros:**\n",
    "- Enables containerization, ensuring reproducibility across different environments.\n",
    "- Simplifies deployment and scaling as containers can run consistently in various environments.\n",
    "- Supports microservices architecture, allowing for modularization of applications.\n",
    "- Easier management of dependencies within containers.\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "Using azure function, lambda functions --> API Gateway\n",
    "\n",
    "### Serverless\n",
    "\n",
    "**Pros:**\n",
    "- Cost-effective as you only pay for actual usage.\n",
    "- Auto-scaling and automatic resource management.\n",
    "- Low operational overhead as the cloud provider handles infrastructure.\n",
    "- Well-suited for event-driven applications and microservices.\n",
    "\n",
    "**Cons:**\n",
    "- Limited control over infrastructure, which may be a limitation for specific requirements.\n",
    "- Cold start latency can impact response times for some functions.\n",
    "- Debugging and monitoring can be more challenging in a serverless environment.\n",
    "\n",
    "\n",
    "### Load Balancer and Server Deployment\n",
    "\n",
    "**Pros:**\n",
    "- Provides control over the underlying infrastructure.\n",
    "- Suitable for applications with specific hardware requirements.\n",
    "- Can be cost-effective for steady-state workloads.\n",
    "- Greater flexibility in configuring load balancing algorithms.\n",
    "\n",
    "\n",
    "**Cons:**\n",
    "- Increased operational complexity compared to serverless or containerized approaches.\n",
    "- Limited scalability during traffic spikes without proper automation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marcimex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
