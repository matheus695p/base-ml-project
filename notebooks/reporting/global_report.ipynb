{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "\n",
    "%load_ext kedro.ipython\n",
    "%reload_kedro ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model namespaces to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.namespaces import NAMESPACES as namespaces\n",
    "\n",
    "namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def get_compile_metric_dataset(namespace, cross_validation_metrics):\n",
    "    metrics = []\n",
    "    for metric, value in cross_validation_metrics.items():\n",
    "        metrics.append([metric, value[\"value\"]])\n",
    "    metrics = (\n",
    "        pd.DataFrame(metrics, columns=[\"metric\", \"value\"])\n",
    "        .set_index(\"metric\")\n",
    "        .T.reset_index(drop=True)\n",
    "    )\n",
    "    metric_columns = list(metrics.columns)\n",
    "    metrics[\"model\"] = namespace\n",
    "    metrics = metrics[[\"model\"] + metric_columns]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "dfs_metrics = []\n",
    "for namespace in namespaces:\n",
    "    model = catalog.load(f\"{namespace}.model_artifact\")\n",
    "    cross_validation_metrics = model.hypertune_results[\"cross_validation_metrics\"]\n",
    "    metrics = get_compile_metric_dataset(namespace, cross_validation_metrics)\n",
    "    dfs_metrics.append(metrics)\n",
    "\n",
    "df_metrics = pd.concat(dfs_metrics, axis=0).reset_index(drop=True)\n",
    "df_metrics_transpose = df_metrics.set_index(\"model\").T\n",
    "df_metrics_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All cross validation metrics visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for col in df_metrics_transpose.columns:\n",
    "    data = df_metrics_transpose[[col]]\n",
    "    data.columns = [\"value\"]\n",
    "    data = data.reset_index()\n",
    "    data[\"model\"] = col\n",
    "    dfs.append(data)\n",
    "\n",
    "df = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "fig = px.bar(df, x=\"metric\", y=\"value\", color=\"model\", barmode=\"group\", height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"accuracy\",\n",
    "    \"f1_weighted\",\n",
    "    \"precision_weighted\",\n",
    "    \"recall_weighted\",\n",
    "    \"roc_auc\",\n",
    "]\n",
    "\n",
    "fig = px.bar(\n",
    "    df[df[\"metric\"].isin(metrics)],\n",
    "    x=\"metric\",\n",
    "    y=\"value\",\n",
    "    color=\"model\",\n",
    "    barmode=\"group\",\n",
    "    height=400,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best and worst models looking at the mean of specified metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_metrics = df[df[\"metric\"].isin(metrics)].groupby(\"model\").mean()\n",
    "df_mean_metrics = df_mean_metrics.sort_values(\n",
    "    \"value\",\n",
    "    ascending=False,\n",
    ")\n",
    "df_mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df_mean_metrics.reset_index(),\n",
    "    x=\"model\",\n",
    "    y=\"value\",\n",
    "    color=\"model\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marcimex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
